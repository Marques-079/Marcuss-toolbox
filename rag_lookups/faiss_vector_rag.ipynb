{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4172634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Retrieval-only RAG (no LLM):\n",
    "- Ingests .txt/.md from ./docs\n",
    "- Chunks, embeds (sentence-transformers)\n",
    "- Stores in FAISS (cosine via inner product on L2-normalized vectors)\n",
    "- Answers by returning top-K chunks and top sentences (extractive)\n",
    "\"\"\"\n",
    "\n",
    "import os, sys, glob, pickle\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "DOCS_DIR = \"./docs\"\n",
    "INDEX_PATH = \"./faiss.index\"\n",
    "META_PATH  = \"./faiss_meta.pkl\"\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "CHUNK_SIZE = 900\n",
    "CHUNK_OVERLAP = 200\n",
    "TOP_K_CHUNKS = 5\n",
    "TOP_K_SENTENCES = 6\n",
    "MMR_LAMBDA = 0.7       # 1.0 = purely relevance, 0 = purely diversity\n",
    "\n",
    "# --------------- Data types -------------\n",
    "@dataclass\n",
    "class ChunkMeta:\n",
    "    source: str\n",
    "    chunk_id: int\n",
    "    text: str\n",
    "\n",
    "# --------------- Utils ------------------\n",
    "def read_texts(dir_path: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Return list of (path, text). Supports .txt, .md.\"\"\"\n",
    "    out = []\n",
    "    for p in glob.glob(os.path.join(dir_path, \"**\", \"*\"), recursive=True):\n",
    "        if os.path.isdir(p):\n",
    "            continue\n",
    "        ext = os.path.splitext(p)[1].lower()\n",
    "        if ext not in {\".txt\", \".md\"}:\n",
    "            continue\n",
    "        try:\n",
    "            with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                t = f.read().strip()\n",
    "                if t:\n",
    "                    out.append((p, t))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return out\n",
    "\n",
    "def chunk_text(t: str, size: int, overlap: int) -> List[str]:\n",
    "    if size <= 0: return [t]\n",
    "    chunks, n, i = [], len(t), 0\n",
    "    while i < n:\n",
    "        j = min(n, i + size)\n",
    "        chunks.append(t[i:j])\n",
    "        if j == n: break\n",
    "        i = max(0, j - overlap)\n",
    "    return chunks\n",
    "\n",
    "def l2_normalize(a: np.ndarray) -> np.ndarray:\n",
    "    n = np.linalg.norm(a, axis=1, keepdims=True) + 1e-12\n",
    "    return (a / n).astype(\"float32\")\n",
    "\n",
    "def embed(model: SentenceTransformer, texts: List[str]) -> np.ndarray:\n",
    "    v = model.encode(texts, batch_size=64, convert_to_numpy=True, show_progress_bar=False)\n",
    "    return l2_normalize(v)\n",
    "\n",
    "# --------------- Index ------------------\n",
    "class VectorIndex:\n",
    "    def __init__(self, dim: int):\n",
    "        self.faiss = faiss.IndexFlatIP(dim)   # inner product on normalized = cosine\n",
    "        self.meta: List[ChunkMeta] = []\n",
    "\n",
    "    def add(self, vecs: np.ndarray, metas: List[ChunkMeta]):\n",
    "        self.faiss.add(vecs)\n",
    "        self.meta.extend(metas)\n",
    "\n",
    "    def search(self, q: np.ndarray, k: int) -> Tuple[np.ndarray, List[ChunkMeta]]:\n",
    "        D, I = self.faiss.search(q, k)\n",
    "        hits = []\n",
    "        for idx in I[0]:\n",
    "            if idx == -1: continue\n",
    "            hits.append(self.meta[idx])\n",
    "        return D[0], hits\n",
    "\n",
    "    def save(self):\n",
    "        faiss.write_index(self.faiss, INDEX_PATH)\n",
    "        with open(META_PATH, \"wb\") as f:\n",
    "            pickle.dump(self.meta, f)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls):\n",
    "        if not (os.path.exists(INDEX_PATH) and os.path.exists(META_PATH)):\n",
    "            return None\n",
    "        idx = faiss.read_index(INDEX_PATH)\n",
    "        with open(META_PATH, \"rb\") as f:\n",
    "            meta = pickle.load(f)\n",
    "        vi = cls(idx.d)\n",
    "        vi.faiss = idx\n",
    "        vi.meta = meta\n",
    "        return vi\n",
    "\n",
    "# ------------- Build / Load -------------\n",
    "def build_or_load_index() -> Tuple[VectorIndex, SentenceTransformer]:\n",
    "    emb_model = SentenceTransformer(EMBED_MODEL)\n",
    "\n",
    "    vi = VectorIndex.load()\n",
    "    if vi:\n",
    "        return vi, emb_model\n",
    "\n",
    "    docs = read_texts(DOCS_DIR)\n",
    "    if not docs:\n",
    "        raise SystemExit(f\"No .txt/.md files found under {DOCS_DIR}\")\n",
    "\n",
    "    vi = VectorIndex(dim=emb_model.get_sentence_embedding_dimension())\n",
    "    metas, all_chunks = [], []\n",
    "\n",
    "    for path, txt in docs:\n",
    "        chunks = chunk_text(txt, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "        for j, ch in enumerate(chunks):\n",
    "            metas.append(ChunkMeta(source=path, chunk_id=j, text=ch))\n",
    "            all_chunks.append(ch)\n",
    "\n",
    "    vecs = embed(emb_model, all_chunks)\n",
    "    vi.add(vecs, metas)\n",
    "    vi.save()\n",
    "    print(f\"[index] built: {len(all_chunks)} chunks from {len(docs)} files\")\n",
    "    return vi, emb_model\n",
    "\n",
    "# ------------- Retrieval core -----------\n",
    "def mmr_select(doc_vecs: np.ndarray, query_vec: np.ndarray, k: int, lam: float=0.7) -> List[int]:\n",
    "    \"\"\"\n",
    "    Maximal Marginal Relevance:\n",
    "    selects k indices balancing relevance to query and diversity among selected.\n",
    "    \"\"\"\n",
    "    if k <= 0: return []\n",
    "    n = doc_vecs.shape[0]\n",
    "    sim_to_q = (doc_vecs @ query_vec.T).ravel()  # cosine (since normalized)\n",
    "    selected, candidates = [], list(range(n))\n",
    "\n",
    "    # pick most relevant first\n",
    "    first = int(np.argmax(sim_to_q))\n",
    "    selected.append(first)\n",
    "    candidates.remove(first)\n",
    "\n",
    "    while len(selected) < min(k, n) and candidates:\n",
    "        # compute diversity term: max similarity to any already-selected doc\n",
    "        sel_vecs = doc_vecs[selected]             # (m, d)\n",
    "        cand_vecs = doc_vecs[candidates]          # (c, d)\n",
    "        # (c, m) similarities\n",
    "        sim_to_sel = cand_vecs @ sel_vecs.T\n",
    "        max_sim_to_sel = sim_to_sel.max(axis=1)\n",
    "\n",
    "        # mmr score\n",
    "        mmr = lam * sim_to_q[candidates] - (1 - lam) * max_sim_to_sel\n",
    "        pick_local = int(np.argmax(mmr))\n",
    "        pick = candidates[pick_local]\n",
    "        selected.append(pick)\n",
    "        candidates.remove(pick)\n",
    "    return selected\n",
    "\n",
    "def split_sentences(text: str) -> List[str]:\n",
    "    # simple sentence splitter; for better results, swap in nltk/syntok\n",
    "    import re\n",
    "    sents = re.split(r'(?<=[\\.\\?\\!])\\s+', text.strip())\n",
    "    return [s for s in sents if s]\n",
    "\n",
    "def top_sentences(emb_model: SentenceTransformer, query: str, passages: List[str], k: int) -> List[Tuple[float, str]]:\n",
    "    # rank individual sentences by cosine similarity\n",
    "    sents = []\n",
    "    for p in passages:\n",
    "        sents.extend(split_sentences(p))\n",
    "\n",
    "    if not sents:\n",
    "        return []\n",
    "\n",
    "    qv = embed(emb_model, [query])          # (1,d)\n",
    "    sv = embed(emb_model, sents)            # (n,d)\n",
    "    scores = (sv @ qv.T).ravel()            # (n,)\n",
    "    idx = np.argsort(-scores)[:k]\n",
    "    return [(float(scores[i]), sents[i]) for i in idx]\n",
    "\n",
    "# ------------- Public API ----------------\n",
    "def retrieve_only(query: str, top_k_chunks=TOP_K_CHUNKS, top_k_sents=TOP_K_SENTENCES):\n",
    "    vi, emb_model = build_or_load_index()\n",
    "\n",
    "    # brute: get a wide set first (e.g., 5x K), then MMR reduce to K\n",
    "    qv = embed(emb_model, [query])          # (1,d)\n",
    "    wide_k = max(top_k_chunks * 5, top_k_chunks)\n",
    "    D, _ = vi.faiss.search(qv, wide_k)\n",
    "    # collect candidate vectors + metas\n",
    "    # We need the vectors for MMR; re-embed the candidate metasâ€™ texts:\n",
    "    # (cheaper: we could cache vectors alongside meta; omitted for brevity)\n",
    "    candidates = vi.meta[: D.shape[1]] if vi.faiss.ntotal == D.shape[1] else vi.meta\n",
    "    cand_texts = [m.text for m in candidates]\n",
    "    cand_vecs = embed(emb_model, cand_texts)\n",
    "\n",
    "    sel_idx = mmr_select(cand_vecs, qv, k=top_k_chunks, lam=MMR_LAMBDA)\n",
    "    chosen = [((cand_vecs[i] @ qv.T).item(), candidates[i]) for i in sel_idx]\n",
    "\n",
    "\n",
    "    # Top sentences (extractive)\n",
    "    passages = [m.text for _, m in chosen]\n",
    "    sent_hits = top_sentences(emb_model, query, passages, k=top_k_sents)\n",
    "\n",
    "    return chosen, sent_hits\n",
    "\n",
    "# ------------- CLI ----------------------\n",
    "def main():\n",
    "    if len(sys.argv) < 2:\n",
    "        print(f\"Usage: {sys.argv[0]} 'your query'\")\n",
    "        sys.exit(1)\n",
    "    query = sys.argv[1]\n",
    "\n",
    "    chunks, sents = retrieve_only(query)\n",
    "\n",
    "    print(\"\\n=== Top Passages ===\")\n",
    "    for rank, (score, meta) in enumerate(chunks, 1):\n",
    "        short = meta.text.replace(\"\\n\", \" \")[:240]\n",
    "        print(f\"{rank:>2}. score={score:.3f}  {meta.source}  [chunk {meta.chunk_id}]\")\n",
    "        print(f\"    {short}...\")\n",
    "    if not chunks:\n",
    "        print(\"No passages found.\")\n",
    "\n",
    "    print(\"\\n=== Top Sentences (extractive) ===\")\n",
    "    for rank, (score, sent) in enumerate(sents, 1):\n",
    "        print(f\"{rank:>2}. {score:.3f}  {sent}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c2c983",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
